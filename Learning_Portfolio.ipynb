{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRWZ4mBJjny9KfML1NK15m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leukschrauber/learning_portfolio/blob/main/Learning_Portfolio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Portfolio\n",
        "*by Fabian Leuk (csba6437/12215478)*"
      ],
      "metadata": {
        "id": "m38ZeRwm7Kjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Session 1\n",
        "\n",
        "### Key Learnings\n",
        "\n",
        "#### Zero-Shot-Learning vs. Few-Shot-Learning\n",
        "\n",
        "One of my key takeaways from the first session is the difference in Zero-Shot-Learning vs. Few-Shot-Learning.\n",
        "\n",
        "**Zero-Shot-Learning** refers to a Machine Learning approach where the model tries to make a correct guess on something that was not included in training data. This works by the availability of related (meta-) information. For example, a model might not have been trained with Tweets. However, it might still be able to recognize something as a Tweet given the information that Tweets are maximum 140 characters long. In this way, the model can distinguish a Tweet from let's say a scientific paper.\n",
        "\n",
        "**Few-Shot-Learning** refers to a Machine Learning approach where the model tries to apply knowledge from training data on a new piece of information. A model that was trained with tweets and scientific papers to distinguish them from one another using a list of examples, will extract relevant meta data like length by itself and thus be able to distinguish.\n",
        "\n",
        "Overall, knowing the difference between the two approaches enables me to write better prompts using ChatGPT or Cohere, as thus far I was unconsciously only applying Zero-Shot-Learning in my prompts. The same approach can also be extended to non-text data such as Images. It also explains to me how a \"Conversation\" with ChatGPT might be programmed in the background with API-Calls.\n",
        "\n",
        "#### Cohere-Client\n",
        "\n",
        "Another key takeaway from the first session was using the Cohere API. The API can be called using a small code snippet. The parameters needed to query a prompt, are described by Cohere as follows. (https://docs.cohere.ai/reference/generate)\n",
        "\n",
        "The Generate endpoint has a number of settings we can use to control the kind of output it generates. The full list is available in the API reference, but let’s look at a few:\n",
        "\n",
        "**model** - Either medium or xlarge. Generally, smaller models are faster while larger models will perform better.\n",
        "\n",
        "**max_tokens** - The maximum length of text to be generated. One word contains approximately 2-3 tokens.\n",
        "\n",
        "**temperature** - Ranges from 0 to 5. Controls the randomness of the output. Lower values tend to generate more “predictable” output, while higher values tend to generate more “creative” output. The sweet spot is typically between 0 and 1.\n",
        "\n",
        "**stop_sequences** - A stop sequence will cut off your generation at the end of the sequence. This effectively informs the model of when to stop. Add your stop sequence at the end of each example in the prompt (refer to the prompt we’d created, which uses “--” as the stop sequence).\n",
        "\n",
        "The following code snippet will be able to call the Cohere API.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "toNRRLd_7TEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere\n",
        "import cohere\n",
        "\n",
        "apikey_cohere = 'TGYoczX3zLdkPtaRIWtyeFgbZQssE7E0BnfP7tPj'\n",
        "\n",
        "def cohere_text(prompt: str, tokens: int) -> str:\n",
        "  co = cohere.Client(apikey_cohere)\n",
        "\n",
        "  response = co.generate(  \n",
        "    model='xlarge',  \n",
        "    prompt = prompt,  \n",
        "    max_tokens=tokens,  \n",
        "    temperature=0.75,  \n",
        "    stop_sequences=[\"\\n\\n\"])\n",
        "\n",
        "  return response.generations[0].text"
      ],
      "metadata": {
        "id": "ldvW1cln95fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application\n",
        "\n",
        "The area of application for Few-Shot-Learning is virtually endless. In the context of Social Media, another Use-Case is the moderation of homophobic content. Based on the decision of the Machine Learning algorithm, a program might flag or delete content. For humans, this work is repetitive, time-expensive and potentially psychologically damaging as agents constantly get exposed to harming content.\n",
        "\n",
        "Thus, I will apply Few-Shot-Learning to classify fictional posts using Few-Shot-Learning in the following code-snippet. The results for 10 Posts are collected and compared to my personal expectations to measure the quality of the model.\n",
        "\n",
        "However, this test is limited due to my personal judgment of the test data set."
      ],
      "metadata": {
        "id": "GQEO3Fjt_tsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip \n",
        "!pip install openai\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "basic_prompt = \"Assume I am a Social Media Company trying to detect homophobic posts in order to flag and delete them.\\n\"\n",
        "\n",
        "train_data_set = \"You're a faggott: Homophobic\\n\" \\\n",
        "\"I like chocolate: OK\\n\" \\\n",
        "\"I don't like chocolate: OK\\n\" \\\n",
        "\"I hate gays: Homophobic\\n\" \\\n",
        "\"Man how do you not like pussy: Homophobic\\n\" \\\n",
        "\"I wish I could be skiing: OK\\n\" \\\n",
        "\"I wish I could beat up this gay: Homophobic\\n\" \\\n",
        "\"Lol get straight faggott: Homophobic\\n\" \\\n",
        "\"Lol I lost 50 perc on my last trade: OK\\n\" \\\n",
        "\"Hi, how are you?: OK\\n\" \\\n",
        "\n",
        "test_data_set = [\n",
        "    [\"Lol Stupid gays\", \"Homophobic\"],\n",
        "    [\"How could you possibly ever love a man?\", \"Homophobic\"],\n",
        "    [\"Get fcked faggott\", \"Homophobic\"],\n",
        "    [\"Queer queen get fckd\", \"Homophobic\"],\n",
        "    [\"Thinking about how gays still are allowed lol\", \"Homophobic\"],\n",
        "    [\"Mama i miss u\", \"OK\"],\n",
        "    [\"Yes I will love a man\", \"OK\"],\n",
        "    [\"I love women\", \"OK\"],\n",
        "    [\"We as a society should agree on non homophobic laws\", \"OK\"],\n",
        "    [\"Gay Marriage should be allowed\", \"OK\"],\n",
        "]\n",
        "\n",
        "def classify_post(post: str):\n",
        "    prompt = \"\"\n",
        "    prompt += basic_prompt\n",
        "    prompt += train_data_set\n",
        "    prompt += post\n",
        "    prompt += \":\"\n",
        "\n",
        "    response = openai.Completion.create(model=\"text-davinci-003\",\n",
        "                                        prompt=prompt, temperature=0,\n",
        "                                        max_tokens=10)\n",
        "\n",
        "    return response[\"choices\"][0][\"text\"]\n"
      ],
      "metadata": {
        "id": "R3NSL4_oC1pO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6116d29-ec72-4e5c-f628-2890861eb223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (23.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (3.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  results = []\n",
        "  correct = 0\n",
        "\n",
        "  for test_data in test_data_set:\n",
        "      classification = classify_post(test_data[0])\n",
        "      results.append(classification.strip())\n",
        "\n",
        "  for index, result in enumerate(results):\n",
        "      if result == test_data_set[index][1]:\n",
        "          correct += 1\n",
        "\n",
        "  print(\"Correct percentage: \" + str(correct/len(test_data_set)*100) + \" %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkKDcu6b_rGM",
        "outputId": "50bcce56-6862-4e73-e8fc-086f47ca4acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct percentage: 90.0 %\n"
          ]
        }
      ]
    }
  ]
}